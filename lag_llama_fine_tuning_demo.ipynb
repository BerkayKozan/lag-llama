{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from gluonts.model.forecast import SampleForecast\n",
    "\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lag_llama.gluon.estimator import LagLlamaEstimator\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select data file to train & test: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"datasets/deterioration/ts_long_0.1_100_20.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set numerical columns as float32\n",
    "for col in df.columns:\n",
    "    # Check if column is not of string type\n",
    "    if df[col].dtype != 'object' and pd.api.types.is_string_dtype(df[col]) == False:\n",
    "        df[col] = df[col].astype('float32')\n",
    "        \n",
    "\n",
    "# Create the Pandas\n",
    "dataset = PandasDataset.from_long_dataframe(df, target=\"target\", item_id=\"item_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Select numerical columns to normalize\n",
    "numerical_columns = df.select_dtypes(include=['float32', 'float64', 'float']).columns\n",
    "\n",
    "# Split the data first\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df = df.iloc[:train_size]\n",
    "# Validation_df: 60-20-20\n",
    "test_df = df.iloc[train_size:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the training data\n",
    "train_df[numerical_columns] = scaler.fit_transform(train_df[numerical_columns])\n",
    "# print(train_df)\n",
    "# print(scaler.mean_)\n",
    "# Normalize the test data using the same scaler (do not fit again)\n",
    "test_df[numerical_columns] = scaler.transform(test_df[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Pandas datasets\n",
    "dataset = {\n",
    "    'train': PandasDataset.from_long_dataframe(train_df, target=\"target\", item_id=\"item_id\"),\n",
    "    'test': PandasDataset.from_long_dataframe(test_df, target=\"target\", item_id=\"item_id\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_length = 10\n",
    "context_length = prediction_length*3\n",
    "num_samples = 10\n",
    "device=torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning \n",
    "Hyper parameters tuning: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
    "estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
    "\n",
    "estimator = LagLlamaEstimator(\n",
    "        ckpt_path=\"lag-llama.ckpt\",\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length,\n",
    "\n",
    "        # distr_output=\"neg_bin\",\n",
    "        # scaling=\"mean\",\n",
    "        nonnegative_pred_samples=True,\n",
    "        aug_prob=0,\n",
    "        lr=5e-4,\n",
    "\n",
    "        # estimator args\n",
    "        input_size=estimator_args[\"input_size\"],\n",
    "        n_layer=estimator_args[\"n_layer\"],\n",
    "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
    "        n_head=estimator_args[\"n_head\"],\n",
    "        time_feat=estimator_args[\"time_feat\"],\n",
    "\n",
    "        # rope_scaling={\n",
    "        #     \"type\": \"linear\",\n",
    "        #     \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
    "        # },\n",
    "        device=device,\n",
    "\n",
    "        batch_size=64,\n",
    "        num_parallel_samples=num_samples,\n",
    "        trainer_kwargs = {\"max_epochs\": 50,}, # <- lightning trainer arguments\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.train(dataset['train'], cache_data=True, shuffle_buffer_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset['test'],\n",
    "        predictor=predictor,\n",
    "        num_samples=num_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = list(tqdm(forecast_it, total=len(dataset), desc=\"Forecasting batches\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(forecasts))\n",
    "print(len(forecasts))\n",
    "print(forecasts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, forecast in enumerate(forecasts):\n",
    "    samples = forecast.samples\n",
    "\n",
    "    # Iterate over each sample and apply the inverse transform\n",
    "    denormalized_samples = np.array([\n",
    "        scaler.inverse_transform(sample.reshape(1, -1)).flatten()\n",
    "        for sample in samples\n",
    "    ])\n",
    "    \n",
    "    # Create a new SampleForecast object with the denormalized samples\n",
    "    denormalized_forecast = SampleForecast(\n",
    "        samples=denormalized_samples,\n",
    "        start_date=forecast.start_date,\n",
    "        item_id=forecast.item_id\n",
    "    )\n",
    "    forecasts[i] = denormalized_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = list(tqdm(ts_it, total=len(dataset), desc=\"Ground truth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tss))\n",
    "print(len(tss[0]))\n",
    "print(type(tss[0]))\n",
    "print(tss[0][0].shape)\n",
    "print(type(tss))\n",
    "print(type(tss[0]))\n",
    "print(tss[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denormalized_tss = []\n",
    "\n",
    "for ts in tss:\n",
    "    ts_copy = ts.copy()\n",
    "    \n",
    "    # Get the values, reshape them to fit the scaler (if necessary)\n",
    "    values = ts_copy.values.reshape(-1, 1)\n",
    "\n",
    "    # Denormalize the values using the scaler\n",
    "    denormalized_values = scaler.inverse_transform(values)\n",
    "\n",
    "    # Update the DataFrame with denormalized values\n",
    "    ts_copy.iloc[:, 0] = denormalized_values.flatten()\n",
    "\n",
    "    # Add the denormalized time series to the list\n",
    "    denormalized_tss.append(ts_copy)\n",
    "\n",
    "tss = denormalized_tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "date_formater = mdates.DateFormatter('%b, %d')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# Iterate through the first 9 series, and plot the predicted samples\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "\n",
    "    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
    "    forecast.plot( color='g')\n",
    "    plt.xticks(rotation=60)\n",
    "    ax.xaxis.set_major_formatter(date_formater)\n",
    "    ax.set_title(forecast.item_id)\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "agg_metrics, ts_metrics = evaluator(iter(tss), iter(forecasts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
